{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap08/8_1_MNIST_1D_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6chybAVFJW2"
   },
   "source": [
    "# **Notebook 8.1: MNIST_1D_Performance**\n",
    "\n",
    "This notebook runs a simple neural network on the MNIST1D dataset as in figure 8.2a. It uses code from https://github.com/greydanus/mnist1d to generate the data.\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ifVjS4cTOqKz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/greydanus/mnist1d\n",
      "  Cloning https://github.com/greydanus/mnist1d to c:\\users\\georg\\appdata\\local\\temp\\pip-req-build-e9w3rjr5\n",
      "  Resolved https://github.com/greydanus/mnist1d to commit 68cb6b2d1d3a956cc2e9035e5a9860b66e33d642\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\georg\\anaconda3\\lib\\site-packages (from mnist1d==0.0.2.post15) (2.31.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\georg\\anaconda3\\lib\\site-packages (from mnist1d==0.0.2.post15) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\georg\\anaconda3\\lib\\site-packages (from mnist1d==0.0.2.post15) (3.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\georg\\anaconda3\\lib\\site-packages (from mnist1d==0.0.2.post15) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from matplotlib->mnist1d==0.0.2.post15) (6.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from requests->mnist1d==0.0.2.post15) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from requests->mnist1d==0.0.2.post15) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from requests->mnist1d==0.0.2.post15) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from requests->mnist1d==0.0.2.post15) (2024.2.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->mnist1d==0.0.2.post15) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mnist1d==0.0.2.post15) (1.16.0)\n",
      "Building wheels for collected packages: mnist1d\n",
      "  Building wheel for mnist1d (pyproject.toml): started\n",
      "  Building wheel for mnist1d (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for mnist1d: filename=mnist1d-0.0.2.post15-py3-none-any.whl size=14653 sha256=6fb79e81a1df3db16af1acac46c15c30d6fdb8fdf3edd46c27f4864661e551f5\n",
      "  Stored in directory: C:\\Users\\georg\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-tarpfjq8\\wheels\\c4\\34\\e0\\a3b4376888d7486638e921a69788a6309c58168eb2b2183b5b\n",
      "Successfully built mnist1d\n",
      "Installing collected packages: mnist1d\n",
      "Successfully installed mnist1d-0.0.2.post15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/greydanus/mnist1d 'C:\\Users\\georg\\AppData\\Local\\Temp\\pip-req-build-e9w3rjr5'\n"
     ]
    }
   ],
   "source": [
    "# Run this if you're in a Colab to install MNIST 1D repository\n",
    "%pip install git+https://github.com/greydanus/mnist1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\georg\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\georg\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\georg\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\georg\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\georg\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.4.1-cp39-cp39-win_amd64.whl (199.3 MB)\n",
      "   ---------------------------------------- 0.0/199.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/199.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/199.3 MB 2.4 MB/s eta 0:01:24\n",
      "   ---------------------------------------- 0.4/199.3 MB 3.6 MB/s eta 0:00:56\n",
      "   ---------------------------------------- 1.0/199.3 MB 6.2 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 2.3/199.3 MB 11.5 MB/s eta 0:00:18\n",
      "    --------------------------------------- 4.0/199.3 MB 17.0 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.6/199.3 MB 19.8 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 7.3/199.3 MB 22.3 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 9.2/199.3 MB 24.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.2/199.3 MB 38.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 11.8/199.3 MB 34.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 13.8/199.3 MB 34.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 15.4/199.3 MB 34.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 17.3/199.3 MB 34.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 18.5/199.3 MB 32.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 20.4/199.3 MB 34.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 22.3/199.3 MB 38.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 24.4/199.3 MB 38.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 27.0/199.3 MB 43.5 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 28.8/199.3 MB 46.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 30.4/199.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 32.2/199.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 33.2/199.3 MB 38.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 34.9/199.3 MB 38.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 37.1/199.3 MB 36.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 39.1/199.3 MB 36.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 41.4/199.3 MB 38.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 42.9/199.3 MB 38.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 44.8/199.3 MB 38.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 46.9/199.3 MB 40.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 49.2/199.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 50.3/199.3 MB 38.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 51.7/199.3 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 53.0/199.3 MB 36.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 54.6/199.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 56.2/199.3 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 58.6/199.3 MB 34.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 60.5/199.3 MB 36.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 62.8/199.3 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 64.7/199.3 MB 40.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 66.8/199.3 MB 43.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 68.3/199.3 MB 40.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 70.5/199.3 MB 40.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 72.9/199.3 MB 43.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 74.8/199.3 MB 43.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 76.3/199.3 MB 43.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 78.6/199.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 80.8/199.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 81.8/199.3 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 83.5/199.3 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 84.3/199.3 MB 36.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 85.7/199.3 MB 32.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 87.5/199.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 89.0/199.3 MB 32.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 90.6/199.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 92.8/199.3 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 94.7/199.3 MB 38.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 96.9/199.3 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 99.1/199.3 MB 40.9 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 101.3/199.3 MB 43.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 103.0/199.3 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 104.9/199.3 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 106.4/199.3 MB 38.6 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 108.0/199.3 MB 38.5 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 110.3/199.3 MB 38.5 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 111.9/199.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 114.4/199.3 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.0/199.3 MB 38.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 117.9/199.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 119.8/199.3 MB 40.9 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 121.5/199.3 MB 38.6 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 122.7/199.3 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.7/199.3 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.7/199.3 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.7/199.3 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 123.5/199.3 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 126.3/199.3 MB 24.3 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 126.8/199.3 MB 24.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.7/199.3 MB 21.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 134.5/199.3 MB 54.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 136.0/199.3 MB 50.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 138.1/199.3 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 139.7/199.3 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 141.4/199.3 MB 50.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 143.4/199.3 MB 43.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 145.5/199.3 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 146.9/199.3 MB 38.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 147.8/199.3 MB 34.6 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 148.9/199.3 MB 32.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 150.3/199.3 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 152.0/199.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 153.4/199.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 155.4/199.3 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 156.8/199.3 MB 31.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 158.7/199.3 MB 36.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 160.8/199.3 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 162.8/199.3 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 164.8/199.3 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 166.1/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 168.0/199.3 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.8/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 171.5/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 172.7/199.3 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 174.6/199.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 176.3/199.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 177.8/199.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 180.2/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 181.6/199.3 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 183.6/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 185.6/199.3 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 187.7/199.3 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 189.3/199.3 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 190.9/199.3 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 192.6/199.3 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  194.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.4/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.3/199.3 MB 38.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 199.3/199.3 MB 14.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7LNq72SP6jO"
   },
   "source": [
    "Let's generate a training and test dataset using the MNIST1D code.  The dataset gets saved as a .pkl file so it doesn't have to be regenerated each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qyE7G1StPIqO"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YLxf7dJfPaqw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did or could not load data from ./mnist1d_data.pkl. Rebuilding dataset...\n",
      "Examples in training set: 4000\n",
      "Examples in test set: 1000\n",
      "Length of each example: 40\n"
     ]
    }
   ],
   "source": [
    "args = mnist1d.data.get_dataset_args()\n",
    "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=False)\n",
    "\n",
    "# The training and test input and outputs are in\n",
    "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
    "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
    "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
    "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FxaB5vc0uevl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_i = 40    # Input dimensions\n",
    "D_k = 100   # Hidden dimensions\n",
    "D_o = 10    # Output dimensions\n",
    "# TO DO:\n",
    "# Define a model with two hidden layers of size 100\n",
    "# And ReLU activations between them\n",
    "# Replace this line (see Figure 7.8 of book for help):\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_i, D_k),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_k, D_k),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D_k, D_o));\n",
    "\n",
    "\n",
    "def weights_init(layer_in):\n",
    "  # TO DO:\n",
    "  # Initialize the parameters with He initialization\n",
    "  # Replace this line (see figure 7.8 of book for help)\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.kaiming_normal_(layer_in.weight)\n",
    "        layer_in.bias.data.fill_(0.0)\n",
    "  print(\"Initializing layer\")\n",
    "\n",
    "\n",
    "# Call the function you just defined\n",
    "model.apply(weights_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_rX6N3VyyQTY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Initializing layer\n",
      "Epoch     0, train loss 1.573109, train error 60.42,  test loss 1.633949, test error 65.50\n",
      "Epoch     1, train loss 1.330980, train error 50.12,  test loss 1.490976, test error 58.80\n",
      "Epoch     2, train loss 1.120854, train error 39.62,  test loss 1.311734, test error 51.60\n",
      "Epoch     3, train loss 1.001093, train error 35.82,  test loss 1.243873, test error 47.10\n",
      "Epoch     4, train loss 0.883473, train error 31.82,  test loss 1.174921, test error 44.60\n",
      "Epoch     5, train loss 0.777452, train error 27.85,  test loss 1.144242, test error 44.30\n",
      "Epoch     6, train loss 0.708802, train error 24.88,  test loss 1.127358, test error 43.20\n",
      "Epoch     7, train loss 0.637505, train error 21.75,  test loss 1.095214, test error 41.40\n",
      "Epoch     8, train loss 0.550880, train error 18.62,  test loss 1.075067, test error 39.40\n",
      "Epoch     9, train loss 0.492412, train error 15.60,  test loss 1.069575, test error 40.10\n",
      "Epoch    10, train loss 0.395839, train error 11.18,  test loss 1.009429, test error 36.70\n",
      "Epoch    11, train loss 0.357307, train error 9.18,  test loss 1.025791, test error 38.20\n",
      "Epoch    12, train loss 0.324799, train error 7.97,  test loss 1.032278, test error 37.60\n",
      "Epoch    13, train loss 0.304938, train error 7.60,  test loss 1.029340, test error 35.30\n",
      "Epoch    14, train loss 0.278440, train error 6.57,  test loss 1.077764, test error 37.80\n",
      "Epoch    15, train loss 0.260618, train error 6.45,  test loss 1.097826, test error 37.20\n",
      "Epoch    16, train loss 0.243349, train error 5.43,  test loss 1.104296, test error 37.90\n",
      "Epoch    17, train loss 0.208665, train error 4.32,  test loss 1.119560, test error 37.20\n",
      "Epoch    18, train loss 0.202434, train error 4.03,  test loss 1.109677, test error 36.40\n",
      "Epoch    19, train loss 0.167972, train error 2.60,  test loss 1.102582, test error 35.70\n",
      "Epoch    20, train loss 0.148279, train error 1.65,  test loss 1.127123, test error 36.60\n",
      "Epoch    21, train loss 0.143382, train error 1.72,  test loss 1.136594, test error 36.50\n",
      "Epoch    22, train loss 0.132448, train error 1.28,  test loss 1.154539, test error 36.20\n",
      "Epoch    23, train loss 0.124344, train error 0.93,  test loss 1.159231, test error 36.90\n",
      "Epoch    24, train loss 0.120333, train error 0.95,  test loss 1.172715, test error 36.60\n",
      "Epoch    25, train loss 0.117176, train error 0.95,  test loss 1.187401, test error 36.30\n",
      "Epoch    26, train loss 0.105982, train error 0.55,  test loss 1.176331, test error 37.40\n",
      "Epoch    27, train loss 0.104907, train error 0.45,  test loss 1.206113, test error 36.20\n",
      "Epoch    28, train loss 0.095946, train error 0.47,  test loss 1.203739, test error 36.90\n",
      "Epoch    29, train loss 0.092347, train error 0.30,  test loss 1.213561, test error 36.60\n",
      "Epoch    30, train loss 0.087886, train error 0.30,  test loss 1.218430, test error 37.60\n",
      "Epoch    31, train loss 0.085770, train error 0.35,  test loss 1.227178, test error 36.70\n",
      "Epoch    32, train loss 0.083621, train error 0.15,  test loss 1.220810, test error 36.60\n",
      "Epoch    33, train loss 0.082525, train error 0.28,  test loss 1.237215, test error 36.70\n",
      "Epoch    34, train loss 0.079639, train error 0.18,  test loss 1.242143, test error 36.80\n",
      "Epoch    35, train loss 0.079125, train error 0.22,  test loss 1.250541, test error 37.10\n",
      "Epoch    36, train loss 0.076568, train error 0.18,  test loss 1.251683, test error 36.60\n",
      "Epoch    37, train loss 0.074336, train error 0.10,  test loss 1.256631, test error 36.50\n",
      "Epoch    38, train loss 0.072409, train error 0.10,  test loss 1.258968, test error 37.10\n",
      "Epoch    39, train loss 0.070935, train error 0.10,  test loss 1.262836, test error 36.40\n",
      "Epoch    40, train loss 0.069898, train error 0.10,  test loss 1.261930, test error 37.10\n",
      "Epoch    41, train loss 0.068885, train error 0.10,  test loss 1.265670, test error 36.90\n",
      "Epoch    42, train loss 0.068287, train error 0.10,  test loss 1.269070, test error 36.80\n",
      "Epoch    43, train loss 0.067586, train error 0.10,  test loss 1.271288, test error 36.60\n",
      "Epoch    44, train loss 0.066931, train error 0.10,  test loss 1.270328, test error 36.80\n",
      "Epoch    45, train loss 0.066009, train error 0.10,  test loss 1.271922, test error 37.00\n",
      "Epoch    46, train loss 0.065445, train error 0.10,  test loss 1.281046, test error 36.60\n",
      "Epoch    47, train loss 0.064552, train error 0.10,  test loss 1.278437, test error 37.00\n",
      "Epoch    48, train loss 0.064123, train error 0.07,  test loss 1.284221, test error 36.40\n",
      "Epoch    49, train loss 0.063488, train error 0.10,  test loss 1.284402, test error 36.80\n"
     ]
    }
   ],
   "source": [
    "# choose cross entropy loss function (equation 5.24)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# construct SGD optimizer and initialize learning rate and momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
    "# object that decreases learning rate by half every 10 epochs\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "x_train = torch.tensor(data['x'].astype('float32'))\n",
    "y_train = torch.tensor(data['y'].transpose().astype('int64'))\n",
    "x_test= torch.tensor(data['x_test'].astype('float32'))\n",
    "y_test = torch.tensor(data['y_test'].astype('int64'))\n",
    "\n",
    "# load the data into a class that creates the batches\n",
    "data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
    "\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "\n",
    "# loop over the dataset n_epoch times\n",
    "n_epoch = 50\n",
    "# store the loss and the % correct at each epoch\n",
    "losses_train = np.zeros((n_epoch))\n",
    "errors_train = np.zeros((n_epoch))\n",
    "losses_test = np.zeros((n_epoch))\n",
    "errors_test = np.zeros((n_epoch))\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  # loop over batches\n",
    "  for i, batch in enumerate(data_loader):\n",
    "    # retrieve inputs and labels for this batch\n",
    "    x_batch, y_batch = batch\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass -- calculate model output\n",
    "    pred = model(x_batch)\n",
    "    # compute the loss\n",
    "    loss = loss_function(pred, y_batch)\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # SGD update\n",
    "    optimizer.step()\n",
    "\n",
    "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
    "  pred_train = model(x_train)\n",
    "  pred_test = model(x_test)\n",
    "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
    "  _, predicted_test_class = torch.max(pred_test.data, 1)\n",
    "  errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
    "  errors_test[epoch]= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)\n",
    "  losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
    "  losses_test[epoch]= loss_function(pred_test, y_test).item()\n",
    "  print(f'Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, train error {errors_train[epoch]:3.2f},  test loss {losses_test[epoch]:.6f}, test error {errors_test[epoch]:3.2f}')\n",
    "\n",
    "  # tell scheduler to consider updating learning rate\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yI-l6kA_EH9G"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors_train,'r-',label='train')\n",
    "ax.plot(errors_test,'b-',label='test')\n",
    "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
    "ax.set_title('TrainError %3.2f, Test Error %3.2f'%(errors_train[-1],errors_test[-1]))\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses_train,'r-',label='train')\n",
    "ax.plot(losses_test,'b-',label='test')\n",
    "ax.set_xlim(0,n_epoch)\n",
    "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "ax.set_title('Train loss %3.2f, Test loss %3.2f'%(losses_train[-1],losses_test[-1]))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-yT6re6GZS4"
   },
   "source": [
    "**TO DO**\n",
    "\n",
    "Play with the model -- try changing the number of layers, hidden units, learning rate, batch size, momentum or anything else you like.  See if you can improve the test results.\n",
    "\n",
    "Is it a good idea to optimize the hyperparameters in this way?  Will the final result be a good estimate of the true test performance?\n",
    "\n",
    "    Currently, this is the ony way to optimize hyperparameters. The final result may not be the best estimate but it'll be a good etimate. Hyperparameter optimization algorithms exist to sample the space of hyperparameters and find the hyperparameters with the best performance, but they are computationally expensive."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOuKMUcKfOIhIL2qTX9jJCy",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
